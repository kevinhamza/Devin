"""
modules/pentesting_tools/web_scanner.py

This module provides tools for scanning and testing the security of web applications.
Features include scanning for vulnerabilities, analyzing HTTP responses, and identifying
security misconfigurations.
"""

import requests
from bs4 import BeautifulSoup
import re


class WebScanner:
    """
    A class for web application scanning and vulnerability detection.
    """

    def __init__(self):
        self.visited_urls = set()

    @staticmethod
    def scan_url(url):
        """
        Scans a single URL for potential security issues.
        :param url: URL to scan.
        :return: Dictionary with scan results.
        """
        try:
            print(f"Scanning URL: {url}")
            response = requests.get(url, timeout=10)
            headers = response.headers
            body = response.text

            vulnerabilities = {
                "xss": WebScanner.check_xss(body),
                "clickjacking": WebScanner.check_clickjacking(headers),
                "sensitive_info": WebScanner.find_sensitive_info(body),
                "open_redirect": WebScanner.check_open_redirect(url),
            }

            print(f"Scan results for {url}: {vulnerabilities}")
            return vulnerabilities

        except Exception as e:
            print(f"Failed to scan URL {url}: {e}")
            return {"error": str(e)}

    @staticmethod
    def check_xss(content):
        """
        Checks for possible XSS vulnerabilities in content.
        :param content: HTML content of the page.
        :return: Boolean indicating if XSS vulnerability is found.
        """
        xss_patterns = [
            r'<script>.*?</script>',
            r'alert\(.*?\)',
            r'<img src=.*? onerror=.*?>',
        ]
        for pattern in xss_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        return False

    @staticmethod
    def check_clickjacking(headers):
        """
        Checks if the site is protected against clickjacking attacks.
        :param headers: HTTP response headers.
        :return: Boolean indicating if clickjacking vulnerability is found.
        """
        return "X-Frame-Options" not in headers

    @staticmethod
    def find_sensitive_info(content):
        """
        Searches for sensitive information in HTML content.
        :param content: HTML content of the page.
        :return: List of detected sensitive information.
        """
        sensitive_keywords = ["password", "secret", "api_key", "token"]
        found_items = [keyword for keyword in sensitive_keywords if keyword in content]
        return found_items

    @staticmethod
    def check_open_redirect(url):
        """
        Checks for open redirect vulnerabilities in the URL.
        :param url: URL to check.
        :return: Boolean indicating if open redirect vulnerability is found.
        """
        if "redirect" in url.lower() or "url" in url.lower():
            return True
        return False

    def crawl_website(self, base_url, max_depth=3, depth=0):
        """
        Crawls a website and scans each page for vulnerabilities.
        :param base_url: Starting URL of the website.
        :param max_depth: Maximum depth to crawl.
        :param depth: Current depth of the crawl.
        :return: Scan report for the entire website.
        """
        if depth > max_depth or base_url in self.visited_urls:
            return

        try:
            print(f"Crawling: {base_url}")
            self.visited_urls.add(base_url)
            response = requests.get(base_url, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")

            # Scan the current URL
            report = {base_url: self.scan_url(base_url)}

            # Find all internal links and recursively crawl them
            for link in soup.find_all("a", href=True):
                href = link["href"]
                if href.startswith("/") or base_url in href:
                    next_url = href if base_url in href else base_url + href
                    report.update(self.crawl_website(next_url, max_depth, depth + 1))

            return report

        except Exception as e:
            print(f"Failed to crawl {base_url}: {e}")
            return {base_url: {"error": str(e)}}


if __name__ == "__main__":
    # Example Usage
    web_scanner = WebScanner()

    # Scan a single URL
    single_url_report = web_scanner.scan_url("https://example.com")
    print(single_url_report)

    # Crawl and scan a website
    website_report = web_scanner.crawl_website("https://example.com", max_depth=2)
    print(website_report)
